{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6044fcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import tarfile\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageOps, ImageFilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10e9c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Compose(object):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img, anno_class_img):\n",
    "        for t in self.transforms:\n",
    "            img, anno_class_img = t(img, anno_class_img)\n",
    "        return img, anno_class_img\n",
    "\n",
    "\n",
    "class Scale(object):\n",
    "    def __init__(self, scale):\n",
    "        self.scale = scale\n",
    "\n",
    "    def __call__(self, img, anno_class_img):\n",
    "\n",
    "        width = img.size[0]\n",
    "        height = img.size[1]\n",
    "        scale = np.random.uniform(self.scale[0], self.scale[1])\n",
    "        scaled_w = int(width * scale)\n",
    "        scaled_h = int(height * scale)\n",
    "        img = img.resize((scaled_w, scaled_h), Image.BICUBIC)\n",
    "        anno_class_img = anno_class_img.resize((scaled_w, scaled_h), Image.NEAREST)\n",
    "        if scale > 1.0:\n",
    "            left = scaled_w - width\n",
    "            left = int(np.random.uniform(0, left))\n",
    "            top = scaled_h-height\n",
    "            top = int(np.random.uniform(0, top))\n",
    "            img = img.crop((left, top, left+width, top+height))\n",
    "            anno_class_img = anno_class_img.crop((left, top, left+width, top+height))\n",
    "        else:\n",
    "            p_palette = anno_class_img.copy().getpalette()\n",
    "            img_original = img.copy()\n",
    "            anno_class_img_original = anno_class_img.copy()\n",
    "            pad_width = width-scaled_w\n",
    "            pad_width_left = int(np.random.uniform(0, pad_width))\n",
    "            pad_height = height-scaled_h\n",
    "            pad_height_top = int(np.random.uniform(0, pad_height))\n",
    "            img = Image.new(img.mode, (width, height), (0, 0, 0))\n",
    "            img.paste(img_original, (pad_width_left, pad_height_top))\n",
    "            anno_class_img = Image.new(anno_class_img.mode, (width, height), (0))\n",
    "            anno_class_img.paste(anno_class_img_original,(pad_width_left, pad_height_top))\n",
    "            anno_class_img.putpalette(p_palette)\n",
    "\n",
    "        return img, anno_class_img\n",
    "\n",
    "\n",
    "class RandomRotation(object):\n",
    "    def __init__(self, angle):\n",
    "        self.angle = angle\n",
    "\n",
    "    def __call__(self, img, anno_class_img):\n",
    "        rotate_angle = (np.random.uniform(self.angle[0], self.angle[1]))\n",
    "        img = img.rotate(rotate_angle, Image.BILINEAR)\n",
    "        anno_class_img = anno_class_img.rotate(rotate_angle, Image.NEAREST)\n",
    "\n",
    "        return img, anno_class_img\n",
    "\n",
    "\n",
    "class RandomMirror(object):\n",
    "    def __call__(self, img, anno_class_img):\n",
    "        if np.random.randint(2):\n",
    "            img = ImageOps.mirror(img)\n",
    "            anno_class_img = ImageOps.mirror(anno_class_img)\n",
    "        return img, anno_class_img\n",
    "\n",
    "\n",
    "class Resize(object):\n",
    "    def __init__(self, input_size):\n",
    "        self.input_size = input_size\n",
    "\n",
    "    def __call__(self, img, anno_class_img):\n",
    "        img = img.resize((self.input_size, self.input_size),Image.BICUBIC)\n",
    "        anno_class_img = anno_class_img.resize((self.input_size, self.input_size), Image.NEAREST)\n",
    "\n",
    "        return img, anno_class_img\n",
    "\n",
    "\n",
    "class Normalize_Tensor(object):\n",
    "    def __init__(self, color_mean, color_std):\n",
    "        self.color_mean = color_mean\n",
    "        self.color_std = color_std\n",
    "\n",
    "    def __call__(self, img, anno_class_img):\n",
    "        img = transforms.functional.to_tensor(img)\n",
    "        img = transforms.functional.normalize(img, self.color_mean, self.color_std)\n",
    "        anno_class_img = np.array(anno_class_img)\n",
    "        index = np.where(anno_class_img == 255)\n",
    "        anno_class_img[index] = 0\n",
    "        anno_class_img = torch.from_numpy(anno_class_img)\n",
    "\n",
    "        return img, anno_class_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfec469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_datapath_list(rootpath):\n",
    "    imgpath_template = os.path.join(rootpath, 'JPEGImages', '%s.jpg')\n",
    "    annopath_template = os.path.join(rootpath, 'SegmentationClass', '%s.png')\n",
    "    \n",
    "    train_id_names = os.path.join(rootpath + 'ImageSets/Segmentation/train.txt')\n",
    "    val_id_names = os.path.join(rootpath + 'ImageSets/Segmentation/val.txt')\n",
    "    \n",
    "    train_img_list = list()\n",
    "    train_anno_list = list()\n",
    "    \n",
    "    for line in open(train_id_names):\n",
    "        file_id = line.strip()\n",
    "        img_path = (imgpath_template % file_id)\n",
    "        anno_path = (annopath_template % file_id)\n",
    "        train_img_list.append(img_path)\n",
    "        train_anno_list.append(anno_path)\n",
    "        \n",
    "    val_img_list = list()\n",
    "    val_anno_list = list()\n",
    "    \n",
    "    for line in open(val_id_names):\n",
    "        file_id = line.strip()\n",
    "        img_path = (imgpath_template % file_id)\n",
    "        anno_path = (annopath_template % file_id)\n",
    "        val_img_list.append(img_path)\n",
    "        val_anno_list.append(anno_path)\n",
    "        \n",
    "    return train_img_list, train_anno_list, val_img_list, val_anno_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e62bb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransform():\n",
    "    def __init__(self, input_size, color_mean, color_std):\n",
    "        self.data_transform = {\n",
    "            'train': Compose([\n",
    "                Scale(scale=[0.5, 1.5]),\n",
    "                RandomRotation(angle=[-10,10]),\n",
    "                RandomMirror(),\n",
    "                Resize(input_size),\n",
    "                Normalize_Tensor(color_mean, color_std),\n",
    "            ]),\n",
    "            'val': Compose([\n",
    "                Resize(input_size),\n",
    "                Normalize_Tensor(color_mean, color_std)\n",
    "            ])\n",
    "        }\n",
    "        \n",
    "    def __call__(self, phase, img, anno_class_img):\n",
    "        return self.data_transform[phase](img, anno_class_img)\n",
    "    \n",
    "\n",
    "class VOCDataset(data.Dataset):\n",
    "    def __init__(self, img_list, anno_list, phase, transform):\n",
    "        self.img_list = img_list\n",
    "        self.anno_list = anno_list\n",
    "        self.phase = phase\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img, anno_class_img = self.pull_item(index)\n",
    "        \n",
    "        return img, anno_class_img\n",
    "    \n",
    "    def pull_item(self, index):\n",
    "        image_file_path = self.img_list[index]\n",
    "        img = Image.open(image_file_path)\n",
    "        anno_file_path = self.anno_list[index]\n",
    "        anno_class_img = Image.open(anno_file_path)\n",
    "        img, anno_class_img = self.transform(self.phase, img, anno_class_img)\n",
    "        \n",
    "        return img, anno_class_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14357c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PSPNet_ResNet50(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(PSPNet_ResNet50, self).__init__()\n",
    "        \n",
    "        block_config = [3, 4, 6, 3]\n",
    "        img_size = 475\n",
    "        img_size_8 = np.ceil(img_size/8).astype(np.int8)\n",
    "        \n",
    "        # encode\n",
    "        self.feature_conv = FeatureMap_convolution()\n",
    "        self.feature_res_1 = ResidualBlockPSP(n_blocks=block_config[0], in_channels=128, mid_channels=64, out_channels=256, stride=1, dilation=1)\n",
    "        self.feature_res_2 = ResidualBlockPSP(n_blocks=block_config[1], in_channels=256, mid_channels=128, out_channels=512, stride=2, dilation=1)\n",
    "        self.feature_dilated_res_1 = ResidualBlockPSP(n_blocks=block_config[2], in_channels=512, mid_channels=256, out_channels=1024, stride=1, dilation=2)\n",
    "        self.feature_dilated_res_2 = ResidualBlockPSP(n_blocks=block_config[3], in_channels=1024, mid_channels=512, out_channels=2048, stride=1, dilation=4)\n",
    "        \n",
    "        # pyramid pooling\n",
    "        self.pyramid_pooling = PyramidPooling(in_channels=2048, pool_sizes=[6, 3, 2, 1], height=img_size_8, width=img_size_8)\n",
    "        \n",
    "        # decoder\n",
    "        self.decode_feature = DecodePSPFeature(height=img_size, width=img_size, n_classes=n_classes)\n",
    "        \n",
    "        # auxloss\n",
    "        self.aux = AuxiliaryPSPlayers(in_channels=1024, height=img_size, width=img_size, n_classes=n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.feature_conv(x)\n",
    "        x = self.feature_res_1(x)\n",
    "        x = self.feature_res_2(x)\n",
    "        x = self.feature_dilated_res_1(x)\n",
    "        output_aux = self.aux(x)\n",
    "        x = self.feature_dilated_res_2(x)\n",
    "        x = self.pyramid_pooling(x)\n",
    "        output = self.decode_feature(x)\n",
    "        \n",
    "        return (output, output_aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd344dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv2DBatchNormRelu(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, dilation, bias):\n",
    "        super(conv2DBatchNormRelu, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, bias=bias)\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.batchnorm(x)\n",
    "        outputs = self.relu(x)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    \n",
    "class FeatureMap_convolution(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureMap_convolution, self).__init__()\n",
    "        \n",
    "        self.cbnr_1 = conv2DBatchNormRelu(3,64,3,2,1,1,False)\n",
    "        self.cbnr_2 = conv2DBatchNormRelu(64,64,3,1,1,1,False)\n",
    "        self.cbnr_3 = conv2DBatchNormRelu(64,128,3,1,1,1,False)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.cbnr_1(x)\n",
    "        x = self.cbnr_2(x)\n",
    "        x = self.cbnr_3(x)\n",
    "        outputs = self.maxpool(x)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47da971",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlockPSP(nn.Sequential):\n",
    "    def __init__(self, n_blocks, in_channels, mid_channels, out_channels, stride, dilation):\n",
    "        super(ResidualBlockPSP, self).__init__()\n",
    "        \n",
    "        self.add_module('block1', bottleNeckPSP(in_channels, mid_channels, out_channels, stride, dilation))\n",
    "        \n",
    "        for i in range(n_blocks-1):\n",
    "            self.add_module(f\"block{i+2}\", bottleNeckIdentifyPSP(out_channels, mid_channels, stride, dilation))\n",
    "            \n",
    "\n",
    "class conv2DBatchNorm(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, dilation, bias):\n",
    "        super(conv2DBatchNorm, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, bias=bias)\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        outputs = self.batchnorm(x)\n",
    "        \n",
    "        return outputs\n",
    "            \n",
    "            \n",
    "class bottleNeckPSP(nn.Module):\n",
    "    def __init__(self, in_channels, mid_channels, out_channels, stride, dilation):\n",
    "        super(bottleNeckPSP, self).__init__()\n",
    "        \n",
    "        self.cbr_1 = conv2DBatchNormRelu(in_channels, mid_channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=False)\n",
    "        self.cbr_2 = conv2DBatchNormRelu(mid_channels, mid_channels, kernel_size=3, stride=stride, padding=dilation, dilation=dilation, bias=False)\n",
    "        self.cb_3 = conv2DBatchNormRelu(mid_channels, out_channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=False)\n",
    "        self.cb_residual = conv2DBatchNorm(in_channels, out_channels, kernel_size=1, stride=stride, padding=0, dilation=1, bias=False)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        conv = self.cb_3(self.cbr_2(self.cbr_1(x)))\n",
    "        residual = self.cb_residual(x)\n",
    "        \n",
    "        return self.relu(conv+residual)\n",
    "    \n",
    "    \n",
    "class bottleNeckIdentifyPSP(nn.Module):\n",
    "    def __init__(self, in_channels, mid_channels, stride, dilation):\n",
    "        super(bottleNeckIdentifyPSP, self).__init__()\n",
    "        \n",
    "        self.cbr_1 = conv2DBatchNormRelu(in_channels, mid_channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=False)\n",
    "        self.cbr_2 = conv2DBatchNormRelu(mid_channels, mid_channels, kernel_size=3, stride=1, padding=dilation, dilation=dilation, bias=False)\n",
    "        self.cb_3 = conv2DBatchNormRelu(mid_channels, in_channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=False)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        conv = self.cb_3(self.cbr_2(self.cbr_1(x)))\n",
    "        residual = x\n",
    "        return self.relu(conv+residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349865e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyramidPooling(nn.Module):\n",
    "    def __init__(self, in_channels, pool_sizes, height, width):\n",
    "        super(PyramidPooling, self).__init__()\n",
    "        \n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        \n",
    "        out_channels = int(in_channels / len(pool_sizes))\n",
    "        \n",
    "        self.avpool_1 = nn.AdaptiveAvgPool2d(output_size=pool_sizes[0])\n",
    "        self.cbr_1 = conv2DBatchNormRelu(in_channels, out_channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=False)\n",
    "        \n",
    "        self.avpool_2 = nn.AdaptiveAvgPool2d(output_size=pool_sizes[1])\n",
    "        self.cbr_2 = conv2DBatchNormRelu(in_channels, out_channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=False)\n",
    "        \n",
    "        self.avpool_3 = nn.AdaptiveAvgPool2d(output_size=pool_sizes[2])\n",
    "        self.cbr_3 = conv2DBatchNormRelu(in_channels, out_channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=False)\n",
    "        \n",
    "        self.avpool_4 = nn.AdaptiveAvgPool2d(output_size=pool_sizes[3])\n",
    "        self.cbr_4 = conv2DBatchNormRelu(in_channels, out_channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out1 = self.cbr_1(self.avpool_1(x))\n",
    "        out1 = F.interpolate(out1, size=(self.height, self.width), mode='bilinear', align_corners=True)\n",
    "        \n",
    "        out2 = self.cbr_2(self.avpool_2(x))\n",
    "        out2 = F.interpolate(out2, size=(self.height, self.width), mode='bilinear', align_corners=True)\n",
    "        \n",
    "        out3 = self.cbr_3(self.avpool_3(x))\n",
    "        out3 = F.interpolate(out3, size=(self.height, self.width), mode='bilinear', align_corners=True)\n",
    "        \n",
    "        out4 = self.cbr_4(self.avpool_4(x))\n",
    "        out4 = F.interpolate(out4, size=(self.height, self.width), mode='bilinear', align_corners=True)\n",
    "   \n",
    "        output = torch.cat([x, out1, out2, out3, out4], dim=1)\n",
    "    \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8328ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecodePSPFeature(nn.Module):\n",
    "    def __init__(self, height, width, n_classes):\n",
    "        super(DecodePSPFeature, self).__init__()\n",
    "        \n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.cbr = conv2DBatchNormRelu(in_channels=4096, out_channels=512, kernel_size=3, stride=1, padding=1, dilation=1, bias=False)\n",
    "        self.dropout = nn.Dropout2d(p=0.1)\n",
    "        self.classification = nn.Conv2d(in_channels=512, out_channels=n_classes, kernel_size=1, stride=1, padding=0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.cbr(x)\n",
    "        x = self.classification(x)\n",
    "        output = F.interpolate(x, size=(self.height, self.width), mode='bilinear', align_corners=True)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    \n",
    "class AuxiliaryPSPlayers(nn.Module):\n",
    "    def __init__(self, in_channels, height, width, n_classes):\n",
    "        super(AuxiliaryPSPlayers, self).__init__()\n",
    "        \n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.cbr = conv2DBatchNormRelu(in_channels=in_channels, out_channels=256, kernel_size=3, stride=1, padding=1, dilation=1, bias=False)\n",
    "        self.dropout = nn.Dropout2d(p=0.1)\n",
    "        self.classification = nn.Conv2d(in_channels=256, out_channels=n_classes, kernel_size=1, stride=1, padding=0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.cbr(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.classification(x)\n",
    "        output = F.interpolate(x, size=(self.height, self.width), mode='bilinear', align_corners=True)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82f7431",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c919f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = PSPNet_ResNet50(n_classes=21)\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f03fda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbce0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/dotdash/data/'\n",
    "weights_dir = '/home/dotdash/weights/'\n",
    "target_path = os.path.join(data_dir, 'VOCtrainval_11-May-2012.tar')\n",
    "url = 'http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar'\n",
    "\n",
    "if not os.path.exists(data_dir):\n",
    "    os.mkdir(data_dir)\n",
    "\n",
    "if not os.path.exists(weights_dir):\n",
    "    os.mkdir(weights_dir)  \n",
    "\n",
    "if not os.path.exists(target_path):\n",
    "    urllib.request.urlretrieve(url, target_path)\n",
    "    tar = tarfile.TarFile(target_path)\n",
    "    tar.extractall(data_dir)\n",
    "    tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653a519a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c6238b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ad7c57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fedcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rootpath = '/home/dotdash/data/VOCdevkit/VOC2012/'\n",
    "train_img_list, train_anno_list, val_img_list, val_anno_list = make_datapath_list(rootpath=rootpath)\n",
    "color_mean = (0.485, 0.456, 0.406)\n",
    "color_std = (0.229, 0.224, 0.225)\n",
    "train_dataset = VOCDataset(\n",
    "    train_img_list, \n",
    "    train_anno_list, \n",
    "    phase='train',\n",
    "    transform=DataTransform(\n",
    "        input_size=475, \n",
    "        color_mean=color_mean,\n",
    "        color_std=color_std\n",
    "    )\n",
    ")\n",
    "val_dataset = VOCDataset(\n",
    "    val_img_list, \n",
    "    val_anno_list, \n",
    "    phase='val',\n",
    "    transform=DataTransform(\n",
    "        input_size=475, \n",
    "        color_mean=color_mean,\n",
    "        color_std=color_std\n",
    "    )\n",
    ")\n",
    "batch_size = 1\n",
    "train_dataloader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "dataloaders_dict = {'train': train_dataloader, 'val': val_dataloader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858eab10",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = PSPNet_ResNet50(n_classes=150)\n",
    "state_dict = torch.load('/home/dotdash/weights/pspnet50_ADE20K.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212a6314",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 21\n",
    "net.decode_feature.classification = nn.Conv2d(in_channels=256, out_channels=n_classes, kernel_size=1, stride=1, padding=0)\n",
    "net.aux.classification = nn.Conv2d(in_channels=256, out_channels=n_classes, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.xavier_normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "net.decode_feature.classification.apply(weights_init)\n",
    "net.aux.classification.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c48e56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PSPLoss(nn.Module):\n",
    "    def __init__(self, aux_weight=0.4):\n",
    "        super(PSPLoss, self).__init__()\n",
    "        self.aux_weight = aux_weight\n",
    "        \n",
    "    def forward(self, outputs, targets):\n",
    "        loss = F.cross_entropy(outputs[0], targets, reduction='mean')\n",
    "        loss_aux = F.cross_entropy(outputs[1], targets, reduction='mean')\n",
    "        \n",
    "        return loss+self.aux_weight*loss_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce796d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = PSPLoss(aux_weight=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031a827e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD([\n",
    "    {'params': net.feature_conv.parameters(), 'lr': 1e-3},\n",
    "    {'params': net.feature_res_1.parameters(), 'lr': 1e-3},\n",
    "    {'params': net.feature_res_2.parameters(), 'lr': 1e-3},\n",
    "    {'params': net.feature_dilated_res_1.parameters(), 'lr': 1e-3},\n",
    "    {'params': net.feature_dilated_res_2.parameters(), 'lr': 1e-3},\n",
    "    {'params': net.pyramid_pooling.parameters(), 'lr': 1e-3},\n",
    "    {'params': net.decode_feature.parameters(), 'lr': 1e-2},\n",
    "    {'params': net.aux.parameters(), 'lr': 1e-2},\n",
    "], momentum=0.9, weight_decay=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78679670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lambda_epoch(epoch):\n",
    "    max_epoch = 30\n",
    "    return math.pow((1-epoch/max_epoch), 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0490c50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084398e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(net, dataloaders_dict, criterion, scheduler, optimizer, num_epochs):\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    net.to(device)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    num_train_imgs = len(dataloaders_dict['train'].dataset)\n",
    "    num_val_imgs = len(dataloaders_dict['val'].dataset)\n",
    "    batch_size = dataloaders_dict['train'].batch_size\n",
    "    \n",
    "    iteration = 1\n",
    "    logs = []\n",
    "    batch_multiplier = 3\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        t_epoch_start = time.time()\n",
    "        t_iter_start = time.time()\n",
    "        epoch_train_loss = 0.0\n",
    "        epoch_val_loss = 0.0\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                net.train()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                print('train')\n",
    "            else:\n",
    "                if ((epoch+1)%5==0):\n",
    "                    net.eval()\n",
    "                    print('val')\n",
    "                else:\n",
    "                    continue\n",
    "            count = 0\n",
    "            for imges, anno_class_imges in dataloaders_dict[phase]:\n",
    "                if imges.size()[0] == 1:\n",
    "                    continue\n",
    "                \n",
    "                imges = imges.to(device)\n",
    "                anno_class_imges = anno_class_imges.to(device)\n",
    "                \n",
    "                if (phase == 'train') and (count == 0):\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    count = batch_multiplier\n",
    "                    \n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = net(imges)\n",
    "                    loss = criterion(outputs, anoo_class_imges.long()) / batch_multiplier\n",
    "                    \n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        count -= 1\n",
    "                        \n",
    "                        if (iteration % 10 == 0):\n",
    "                            t_iter_finish = time.time()\n",
    "                            duration = t_iter_finish - t_iter_start\n",
    "                            print(f\"iteration {iteration} | Loss: {loss.item()/batch_size+batch_multiplier:.4f} | 10iter: {duration} sec.\")\n",
    "                            t_iter_start = time.time()\n",
    "                            \n",
    "                        epoch_train_loss += loss.item() * batch_multiplier\n",
    "                        iteration += 1\n",
    "                    else:\n",
    "                        epoch_val_loss += loss.item() * batch_multiplier\n",
    "                        \n",
    "        t_epoch_finish = time.time()\n",
    "        print(f\"epoch {epoch+1} | Epoch_Train_Loss: {epoch_train_loss/num_train_imgs:.4f} | Epoch_Val_Loss: {epoch_val_loss/num_val_imgs:.4f} | timer: {t_epoch_finish-t_epoch_start:.4f}\")\n",
    "        \n",
    "        log_epoch = {'epoch': epoch+1, 'train_loss': epoch_train_loss/num_train_imgs, 'val_loss': epoch_val_loss/num_val_imgs}\n",
    "        logs.append(log_epoch)\n",
    "        df = pd.DataFrame(logs)\n",
    "        df.to_csv('log_output.csv')\n",
    "    \n",
    "    torch.save(net.state_dict(), f\"weights/pspnet50_{epoch+1}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7b5f05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# My GPU is not powerful enough. \n",
    "num_epochs = 2\n",
    "train_model(net, dataloaders_dict, criterion, scheduler, optimizer, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e999bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2c3b4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b900c088",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18b391a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e319523",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PSPNet_ResNet50(n_classes=21)\n",
    "state_dict = torch.load(\"/home/dotdash/weights/pspnet50_30.pth\",\n",
    "                        map_location={'cuda:0': 'cpu'})\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "batch_size, channel_size, height, width = 1, 3, 475, 475\n",
    "x = torch.randn(batch_size, channel_size, height, width, requires_grad=True)\n",
    "torch_out = model(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4511d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(\n",
    "    model, # model being run \n",
    "    x, # model input (or a tuple for multiple inputs) \n",
    "    '/home/dotdash/weights/temp.onnx', # where to save the model  \n",
    "    export_params=True, # store the trained parameter weights inside the model file \n",
    "    opset_version=10, # the ONNX version to export the model to \n",
    "    do_constant_folding=True, # whether to execute constant folding for optimization \n",
    "    input_names = ['modelInput'], # the model's input names \n",
    "    output_names = ['modelOutput'], # the model's output names \n",
    "    dynamic_axes={'modelInput' : {0 : 'batch_size'},'modelOutput' : {0 : 'batch_size'}} # variable length axes \n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbc50d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
